{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e41cf45437bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m \u001b[0mdf_adjusted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassified\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjust_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./2010_1to2019_12.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_adjusted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassified\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-e41cf45437bc>\u001b[0m in \u001b[0;36madjust_data\u001b[1;34m(dataname)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madjust_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Datetime'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2220\u001b[0m             \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2221\u001b[1;33m             \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2223\u001b[0m         \u001b[1;31m# maybe create a mi on the columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_index\u001b[1;34m(self, data, alldata, columns, indexnamerow)\u001b[0m\n\u001b[0;32m   1665\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_complex_date_col\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_simple_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agg_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_complex_date_col\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_processed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_agg_index\u001b[1;34m(self, index, try_parse_dates)\u001b[0m\n\u001b[0;32m   1758\u001b[0m                     )\n\u001b[0;32m   1759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m             \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_na_values\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mcol_na_fvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_infer_types\u001b[1;34m(self, values, na_values, try_num_bool)\u001b[0m\n\u001b[0;32m   1875\u001b[0m                 \u001b[1;31m#  TypeError can be raised in floatify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1877\u001b[1;33m                 \u001b[0mna_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msanitize_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1878\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m                 \u001b[0mna_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\"hour_range\"\n",
    "        \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\",\n",
    "        \"10m_rate\",\n",
    "        \"20m_rate\",\n",
    "        \"30m_rate\",\n",
    "        \"40m_rate\",\n",
    "        \"50m_rate\",\n",
    "        \"60m_rate\",\n",
    "        \"70m_rate\",\n",
    "        \"80m_rate\",\n",
    "        \"90m_rate\",\n",
    "        \"100m_rate\",\n",
    "        \"110m_rate\",\n",
    "        \"120m_rate\",\n",
    "        \"130m_rate\",\n",
    "        \"140m_rate\",\n",
    "        \"150m_rate\",\n",
    "        \"160m_rate\",\n",
    "        \"170m_rate\",\n",
    "        \"180m_rate\",\n",
    "        \"190m_rate\",\n",
    "        \"200m_rate\",\n",
    "        \"210m_rate\",\n",
    "        \"220m_rate\",\n",
    "        \"230m_rate\",\n",
    "        \"240m_rate\",\n",
    "        \"250m_rate\",\n",
    "        \"260m_rate\",\n",
    "        \"270m_rate\",\n",
    "        \"280m_rate\",\n",
    "        \"290m_rate\",\n",
    "        \"300m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"January\"] = df[\"January\"]\n",
    "    df_feature[\"February\"] = df[\"February\"]\n",
    "    df_feature[\"March\"] = df[\"March\"]\n",
    "    df_feature[\"April\"] = df[\"April\"]\n",
    "    df_feature[\"May\"] = df[\"May\"]\n",
    "    df_feature[\"June\"] = df[\"June\"]\n",
    "    df_feature[\"July\"] = df[\"July\"]\n",
    "    df_feature[\"August\"] = df[\"August\"]\n",
    "    df_feature[\"September\"] = df[\"September\"]\n",
    "    df_feature[\"October\"] = df[\"October\"]\n",
    "    df_feature[\"November\"] = df[\"November\"]\n",
    "    df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "    df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "    df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "    df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "    df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "    df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "    df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "    df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    df_feature[\"hour_range\"] = df[\"hour_range\"]\n",
    "    \n",
    "    i = 10\n",
    "    while i != 300 + 10:\n",
    "        df_feature[str(i)+\"m_rate\"] = df[\"Close\"].pct_change(int(i/10))\n",
    "        i += 10\n",
    "#     df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()\n",
    "#     df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)\n",
    "#     df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)\n",
    "#     df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "    c=[0,0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        percent = 0.05\n",
    "\n",
    "        if x<-percent:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif -percent<x<percent:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "        elif percent<x:\n",
    "            c[2]+=1\n",
    "            return 2\n",
    "       \n",
    "\n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = df_feature[\"120m_rate\"].apply(lambda x: classify(x))\n",
    "\n",
    "    classified = classified.shift(-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    classified=classified[30:len(classified)-2]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.8)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "\n",
    "#print(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "#X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "#print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "import pickle\n",
    "filename = 'RF_01.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "#print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111734, 147110, 117415]\n",
      "0.43957153868644183\n",
      "[[ 6250 10666  5363]\n",
      " [ 4798 19504  5162]\n",
      " [ 4912 11269  7322]]\n",
      "[0.39160401 0.47066773 0.41026503]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\n",
    "        \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\",\n",
    "        \"10m_rate\",\n",
    "        \"30m_rate\",\n",
    "        \"60m_rate\",\n",
    "        \"120m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"January\"] = df[\"January\"]\n",
    "    df_feature[\"February\"] = df[\"February\"]\n",
    "    df_feature[\"March\"] = df[\"March\"]\n",
    "    df_feature[\"April\"] = df[\"April\"]\n",
    "    df_feature[\"May\"] = df[\"May\"]\n",
    "    df_feature[\"June\"] = df[\"June\"]\n",
    "    df_feature[\"July\"] = df[\"July\"]\n",
    "    df_feature[\"August\"] = df[\"August\"]\n",
    "    df_feature[\"September\"] = df[\"September\"]\n",
    "    df_feature[\"October\"] = df[\"October\"]\n",
    "    df_feature[\"November\"] = df[\"November\"]\n",
    "    df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "    df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "    df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "    df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "    df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "    df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "    df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "    df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()*100\n",
    "    df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)*100\n",
    "    df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)*100\n",
    "    df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)*100\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "    c=[0,0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        percent = 0.05\n",
    "\n",
    "        if x<-percent:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif -percent<x<percent:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "        elif percent<x:\n",
    "            c[2]+=1\n",
    "            return 2\n",
    "       \n",
    "\n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = df_feature[\"120m_rate\"].apply(lambda x: classify(x))\n",
    "\n",
    "    #classified = classified.shift(-1)\n",
    "    classified = classified.shift(-12)\n",
    "    \n",
    "    \n",
    "    #df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    #classified=classified[30:len(classified)-2]\n",
    "    df_adjusted=df_feature[30:len(df_feature)-12]\n",
    "    classified=classified[30:len(classified)-12]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.8)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "\n",
    "#print(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "#X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "#print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "import pickle\n",
    "filename = 'RF_01.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "#print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111734, 147110, 117415]\n",
      "|   iter    |  target   | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "|  1        |  0.4325   |  0.7677   |  206.1    |  399.2    |\n",
      "|  2        |  0.433    |  0.2855   |  161.3    |  379.8    |\n",
      "|  3        |  0.4331   |  0.2617   |  31.99    |  127.6    |\n",
      "|  4        |  0.436    |  0.1338   |  76.52    |  256.1    |\n",
      "|  5        |  0.4325   |  0.2854   |  110.9    |  92.33    |\n",
      "|  6        |  0.4327   |  0.5311   |  18.44    |  288.4    |\n",
      "|  7        |  0.4323   |  0.5797   |  75.79    |  256.4    |\n",
      "|  8        |  0.4305   |  0.7171   |  168.0    |  54.45    |\n",
      "|  9        |  0.4356   |  0.1357   |  89.49    |  200.1    |\n",
      "|  10       |  0.4267   |  0.1632   |  196.5    |  14.2     |\n",
      "|  11       |  0.4324   |  0.5956   |  231.4    |  304.3    |\n",
      "|  12       |  0.4336   |  0.3083   |  18.44    |  268.0    |\n",
      "|  13       |  0.4337   |  0.5948   |  45.92    |  425.7    |\n",
      "|  14       |  0.4334   |  0.442    |  93.91    |  421.4    |\n",
      "|  15       |  0.4309   |  0.7544   |  14.52    |  149.2    |\n",
      "|  16       |  0.4336   |  0.2322   |  139.1    |  392.8    |\n",
      "|  17       |  0.4347   |  0.3975   |  30.54    |  429.1    |\n",
      "|  18       |  0.4334   |  0.7899   |  39.42    |  414.9    |\n",
      "|  19       |  0.4321   |  0.7268   |  213.9    |  261.3    |\n",
      "|  20       |  0.4327   |  0.3866   |  84.3     |  225.1    |\n",
      "|  21       |  0.4324   |  0.9164   |  127.9    |  424.8    |\n",
      "|  22       |  0.4338   |  0.2037   |  169.6    |  428.1    |\n",
      "|  23       |  0.4285   |  0.4833   |  250.0    |  21.05    |\n",
      "|  24       |  0.433    |  0.73     |  84.87    |  359.9    |\n",
      "|  25       |  0.4326   |  0.1504   |  253.5    |  116.0    |\n",
      "|  26       |  0.4178   |  0.6372   |  11.92    |  28.2     |\n",
      "|  27       |  0.4337   |  0.2814   |  77.03    |  255.0    |\n",
      "|  28       |  0.4334   |  0.2904   |  87.62    |  200.5    |\n",
      "|  29       |  0.4321   |  0.5976   |  91.52    |  200.5    |\n",
      "|  30       |  0.4406   |  0.1016   |  30.22    |  431.1    |\n",
      "|  31       |  0.4347   |  0.4104   |  30.66    |  433.5    |\n",
      "|  32       |  0.4342   |  0.322    |  32.63    |  429.7    |\n",
      "|  33       |  0.4328   |  0.4012   |  78.72    |  257.2    |\n",
      "|  34       |  0.4331   |  0.7505   |  28.09    |  432.3    |\n",
      "|  35       |  0.4335   |  0.7146   |  30.96    |  432.7    |\n",
      "|  36       |  0.4393   |  0.1381   |  31.43    |  430.9    |\n",
      "|  37       |  0.4344   |  0.4495   |  29.57    |  430.7    |\n",
      "|  38       |  0.4333   |  0.2446   |  89.31    |  201.6    |\n",
      "|  39       |  0.4345   |  0.3249   |  30.33    |  431.4    |\n",
      "|  40       |  0.4315   |  0.6916   |  96.76    |  94.9     |\n",
      "|  41       |  0.4323   |  0.6811   |  138.5    |  393.3    |\n",
      "|  42       |  0.4332   |  0.4137   |  170.1    |  428.3    |\n",
      "|  43       |  0.4295   |  0.7003   |  297.2    |  23.47    |\n",
      "|  44       |  0.4328   |  0.1783   |  214.4    |  111.3    |\n",
      "|  45       |  0.4221   |  0.416    |  90.65    |  13.37    |\n",
      "|  46       |  0.4345   |  0.3571   |  40.35    |  414.5    |\n",
      "|  47       |  nan      |  0.2014   |  18.56    |  424.7    |\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 \u001b[0mx_probe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Queue is empty, no more objects to retrieve.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1205686c32e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[0mgp_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"alpha\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[0mrandomforest_cv_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgp_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandomforest_cv_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandomforest_cv_bo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mx_probe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(self, utility_function)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Finding argmax of the acquisition function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_vector_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             X, y = self._validate_data(X, y, multi_output=True, y_numeric=True,\n\u001b[0m\u001b[0;32m    190\u001b[0m                                        ensure_2d=True, dtype=\"numeric\")\n\u001b[0;32m    191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    802\u001b[0m                     estimator=estimator)\n\u001b[0;32m    803\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0m\u001b[0;32m    805\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[0;32m    806\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\predict_rate\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\n",
    "        \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\",\n",
    "        \"10m_rate\",\n",
    "        \"30m_rate\",\n",
    "        \"60m_rate\",\n",
    "        \"120m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"January\"] = df[\"January\"]\n",
    "    df_feature[\"February\"] = df[\"February\"]\n",
    "    df_feature[\"March\"] = df[\"March\"]\n",
    "    df_feature[\"April\"] = df[\"April\"]\n",
    "    df_feature[\"May\"] = df[\"May\"]\n",
    "    df_feature[\"June\"] = df[\"June\"]\n",
    "    df_feature[\"July\"] = df[\"July\"]\n",
    "    df_feature[\"August\"] = df[\"August\"]\n",
    "    df_feature[\"September\"] = df[\"September\"]\n",
    "    df_feature[\"October\"] = df[\"October\"]\n",
    "    df_feature[\"November\"] = df[\"November\"]\n",
    "    df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "    df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "    df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "    df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "    df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "    df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "    df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "    df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()*100\n",
    "    df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)*100\n",
    "    df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)*100\n",
    "    df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)*100\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "    c=[0,0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        percent = 0.05\n",
    "\n",
    "        if x<-percent:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif -percent<x<percent:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "        elif percent<x:\n",
    "            c[2]+=1\n",
    "            return 2\n",
    "       \n",
    "\n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = df_feature[\"120m_rate\"].apply(lambda x: classify(x))\n",
    "\n",
    "    #classified = classified.shift(-1)\n",
    "    classified = classified.shift(-12)\n",
    "    \n",
    "    \n",
    "    #df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    #classified=classified[30:len(classified)-2]\n",
    "    df_adjusted=df_feature[30:len(df_feature)-12]\n",
    "    classified=classified[30:len(classified)-12]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.7)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "\n",
    "#print(X_train, y_train)\n",
    "\n",
    "search_params = {\n",
    "     'n_estimators'      : [5, 10, 20, 30, 50, 100, 300],\n",
    "      'max_features'      : [3, 5, 10, 15, 20],\n",
    "      'random_state'      : [2525],\n",
    "      'n_jobs'            : [1],\n",
    "      'min_samples_split' : [3, 5, 10, 15, 20, 25, 30, 40, 50, 100],\n",
    "      'max_depth'         : [3, 5, 10, 15, 20, 25, 30, 40, 50, 100]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "def randomforest_cv(n_estimators, min_samples_split, max_features):\n",
    "    val = cross_val_score(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=max_features,\n",
    "            random_state=0\n",
    "        ),\n",
    "        X_train, y_train,\n",
    "        scoring = 'accuracy',\n",
    "        cv = 3, # 3-fold\n",
    "        n_jobs = -1 # use all CPUs\n",
    "    ).mean()\n",
    "    return val\n",
    "\n",
    "randomforest_cv_bo = BayesianOptimization(\n",
    "    randomforest_cv,\n",
    "    {'n_estimators': (10, 500),\n",
    "    'min_samples_split': (2, 300),\n",
    "    'max_features': (0.1, 0.999)}\n",
    ")\n",
    "\n",
    "gp_params = {\"alpha\": 1e-5}\n",
    "randomforest_cv_bo.maximize(n_iter=50, **gp_params)\n",
    "print(randomforest_cv_bo.res['max']['max_val'])\n",
    "print(randomforest_cv_bo.res['max']['max_params'])\n",
    "\n",
    "\n",
    "\n",
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# #X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "# #print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "# from sklearn.metrics import precision_score\n",
    "# print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "# import pickle\n",
    "# filename = 'RF_01.sav'\n",
    "# pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "#print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81527, 106937, 103355, 84449]\n",
      "class    0\n",
      "dtype: int64\n",
      "0.5249308951732936\n",
      "[[ 9873  4209  1250  1015]\n",
      " [ 3881 10375  5100  1851]\n",
      " [ 1781  6029  9164  3913]\n",
      " [ 1083  1666  3970 10088]]\n",
      "[0.59411482 0.46568517 0.47033463 0.59809095]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\n",
    "        \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\",\n",
    "        \"10m_rate\",\n",
    "        \"30m_rate\",\n",
    "        \"60m_rate\",\n",
    "        \"120m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"January\"] = df[\"January\"]\n",
    "    df_feature[\"February\"] = df[\"February\"]\n",
    "    df_feature[\"March\"] = df[\"March\"]\n",
    "    df_feature[\"April\"] = df[\"April\"]\n",
    "    df_feature[\"May\"] = df[\"May\"]\n",
    "    df_feature[\"June\"] = df[\"June\"]\n",
    "    df_feature[\"July\"] = df[\"July\"]\n",
    "    df_feature[\"August\"] = df[\"August\"]\n",
    "    df_feature[\"September\"] = df[\"September\"]\n",
    "    df_feature[\"October\"] = df[\"October\"]\n",
    "    df_feature[\"November\"] = df[\"November\"]\n",
    "    df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "    df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "    df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "    df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "    df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "    df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "    df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "    df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()*100\n",
    "    df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)*100\n",
    "    df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)*100\n",
    "    df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)*100\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "    c=[0,0,0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        percent = 0.04\n",
    "\n",
    "        if x<-percent:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif -percent<x<=0:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "        elif 0<x<percent:\n",
    "            c[2]+=1\n",
    "            return 2\n",
    "        elif percent<x:\n",
    "            c[3]+=1\n",
    "            return 3\n",
    "       \n",
    "\n",
    "    legs = 30\n",
    "    \n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = df_feature[str(legs) + \"m_rate\"].apply(lambda x: classify(x))\n",
    "\n",
    "    #classified = classified.shift(-1)\n",
    "    classified = classified.shift(-1)\n",
    "    \n",
    "    \n",
    "    #df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    #classified=classified[30:len(classified)-2]\n",
    "    df_adjusted=df_feature[30:len(df_feature)-int(legs/10)]\n",
    "    classified=classified[30:len(classified)-int(legs/10)]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    print(classified.isnull().sum())\n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.8)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "#print(y_train.isnull().any())\n",
    "#print(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "#X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "#print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "import pickle\n",
    "filename = 'RF_01.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "#print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182900, 193365]\n",
      "class    0\n",
      "dtype: int64\n",
      "0.5464005209510013\n",
      "[[18239 18656]\n",
      " [15476 22876]]\n",
      "[0.54097583 0.5508042 ]\n",
      "[0.03089567 0.00225083 0.00227094 0.0023678  0.00237271 0.00242215\n",
      " 0.00243017 0.00238422 0.00236787 0.00237018 0.0024425  0.00235882\n",
      " 0.00225201 0.02260825 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.02480127 0.02403885 0.02730564\n",
      " 0.02501402 0.02459327 0.02484997 0.02734432 0.02694795 0.02631672\n",
      " 0.02609477 0.02595314 0.02622143 0.02595873 0.02591201 0.02570516\n",
      " 0.02584274 0.025679   0.0258628  0.02577572 0.02580065 0.02573175\n",
      " 0.02551259 0.02552885 0.02544952 0.02522874 0.0254438  0.02485289\n",
      " 0.02513222 0.02489566 0.02493321 0.02477481 0.02464281 0.02459323\n",
      " 0.02481125 0.02500072 0.02565568]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"Close\",\n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\n",
    "        \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\",\n",
    "        \"10m_rate\",\n",
    "        \"20m_rate\",\n",
    "        \"30m_rate\",\n",
    "        \"40m_rate\",\n",
    "        \"50m_rate\",\n",
    "        \"60m_rate\",\n",
    "        \"70m_rate\",\n",
    "        \"80m_rate\",\n",
    "        \"90m_rate\",\n",
    "        \"100m_rate\",\n",
    "        \"110m_rate\",\n",
    "        \"120m_rate\",\n",
    "        \"130m_rate\",\n",
    "        \"140m_rate\",\n",
    "        \"150m_rate\",\n",
    "        \"160m_rate\",\n",
    "        \"170m_rate\",\n",
    "        \"180m_rate\",\n",
    "        \"190m_rate\",\n",
    "        \"200m_rate\",\n",
    "        \"210m_rate\",\n",
    "        \"220m_rate\",\n",
    "        \"230m_rate\",\n",
    "        \"240m_rate\",\n",
    "        \"250m_rate\",\n",
    "        \"260m_rate\",\n",
    "        \"270m_rate\",\n",
    "        \"280m_rate\",\n",
    "        \"290m_rate\",\n",
    "        \"300m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"Close\"] = df[\"Close\"]\n",
    "    \n",
    "    df_feature[\"January\"] = df[\"January\"]\n",
    "    df_feature[\"February\"] = df[\"February\"]\n",
    "    df_feature[\"March\"] = df[\"March\"]\n",
    "    df_feature[\"April\"] = df[\"April\"]\n",
    "    df_feature[\"May\"] = df[\"May\"]\n",
    "    df_feature[\"June\"] = df[\"June\"]\n",
    "    df_feature[\"July\"] = df[\"July\"]\n",
    "    df_feature[\"August\"] = df[\"August\"]\n",
    "    df_feature[\"September\"] = df[\"September\"]\n",
    "    df_feature[\"October\"] = df[\"October\"]\n",
    "    df_feature[\"November\"] = df[\"November\"]\n",
    "    df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "    df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "    df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "    df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "    df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "    df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "    df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "    df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    \n",
    "    i = 10\n",
    "    while i != 300 + 10:\n",
    "        df_feature[str(i)+\"m_rate\"] = df[\"Close\"].pct_change(int(i/10))*100\n",
    "        i += 10\n",
    "#     df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()*100\n",
    "#     df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)*100\n",
    "#     df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)*100\n",
    "#     df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)*100\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "#     c=[0,0,0,0]\n",
    "    \n",
    "    \n",
    "#     def classify(x):\n",
    "#         percent = 0.06\n",
    "\n",
    "#         if x<-percent:\n",
    "#             c[0]+=1\n",
    "#             return 0\n",
    "#         elif -percent<x<=0:\n",
    "#             c[1]+=1\n",
    "#             return 1\n",
    "#         elif 0<x<percent:\n",
    "#             c[2]+=1\n",
    "#             return 2\n",
    "#         elif percent<x:\n",
    "#             c[3]+=1\n",
    "#             return 3\n",
    "       \n",
    "    \n",
    "#     c=[0,0,0]\n",
    "    \n",
    "    \n",
    "#     def classify(x):\n",
    "#         percent = 0.06\n",
    "\n",
    "#         if x<-percent:\n",
    "#             c[0]+=1\n",
    "#             return 0\n",
    "#         elif -percent<x<=percent:\n",
    "#             c[1]+=1\n",
    "#             return 1\n",
    "#         elif percent<x:\n",
    "#             c[2]+=1\n",
    "#             return 2\n",
    "       \n",
    "    \n",
    "    \n",
    "    c=[0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        #percent = 0.06\n",
    "\n",
    "        if x<0:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif 0<=x:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "\n",
    "        \n",
    "        \n",
    "    legs = 60\n",
    "    \n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = df_feature[str(legs) + \"m_rate\"].apply(lambda x: classify(x))\n",
    "\n",
    "    #classified = classified.shift(-1)\n",
    "    classified = classified.shift(-int(legs/10))\n",
    "    \n",
    "    \n",
    "    #df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    #classified=classified[30:len(classified)-2]\n",
    "    df_adjusted=df_feature[30:len(df_feature)-int(legs/10)]\n",
    "    classified=classified[30:len(classified)-int(legs/10)]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    print(classified.isnull().sum())\n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.8)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "#print(y_train.isnull().any())\n",
    "#print(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "#X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "#print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "import pickle\n",
    "filename = 'RF_01.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182900, 193365]\n",
      "class    0\n",
      "dtype: int64\n",
      "0.640942496046354\n",
      "[[22411 14137]\n",
      " [12881 25818]]\n",
      "[0.63501643 0.64617695]\n",
      "[0.13409947 0.00532727 0.00521763 0.00537635 0.00493832 0.00493109\n",
      " 0.00481224 0.00531981 0.00501303 0.00522609 0.00544647 0.00423991\n",
      " 0.00424521 0.08500023 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.11817354 0.11865165 0.12266496\n",
      " 0.12003965 0.1207126  0.12056447]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"Close\",\n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\n",
    "        \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\"#,\n",
    "#         \"10m_rate\",\n",
    "#         \"20m_rate\",\n",
    "#         \"30m_rate\",\n",
    "#         \"40m_rate\",\n",
    "#         \"50m_rate\",\n",
    "#         \"60m_rate\",\n",
    "#         \"70m_rate\",\n",
    "#         \"80m_rate\",\n",
    "#         \"90m_rate\",\n",
    "#         \"100m_rate\",\n",
    "#         \"110m_rate\",\n",
    "#         \"120m_rate\",\n",
    "#         \"130m_rate\",\n",
    "#         \"140m_rate\",\n",
    "#         \"150m_rate\",\n",
    "#         \"160m_rate\",\n",
    "#         \"170m_rate\",\n",
    "#         \"180m_rate\",\n",
    "#         \"190m_rate\",\n",
    "#         \"200m_rate\",\n",
    "#         \"210m_rate\",\n",
    "#         \"220m_rate\",\n",
    "#         \"230m_rate\",\n",
    "#         \"240m_rate\",\n",
    "#         \"250m_rate\",\n",
    "#         \"260m_rate\",\n",
    "#         \"270m_rate\",\n",
    "#         \"280m_rate\",\n",
    "#         \"290m_rate\",\n",
    "#         \"300m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"Close\"] = df[\"Close\"]\n",
    "    \n",
    "    df_feature[\"January\"] = df[\"January\"]\n",
    "    df_feature[\"February\"] = df[\"February\"]\n",
    "    df_feature[\"March\"] = df[\"March\"]\n",
    "    df_feature[\"April\"] = df[\"April\"]\n",
    "    df_feature[\"May\"] = df[\"May\"]\n",
    "    df_feature[\"June\"] = df[\"June\"]\n",
    "    df_feature[\"July\"] = df[\"July\"]\n",
    "    df_feature[\"August\"] = df[\"August\"]\n",
    "    df_feature[\"September\"] = df[\"September\"]\n",
    "    df_feature[\"October\"] = df[\"October\"]\n",
    "    df_feature[\"November\"] = df[\"November\"]\n",
    "    df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "    df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "    df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "    df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "    df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "    df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "    df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "    df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    \n",
    "#     i = 10\n",
    "#     while i != 300 + 10:\n",
    "#         df_feature[str(i)+\"m_rate\"] = df[\"Close\"].pct_change(int(i/10))*100\n",
    "#         i += 10\n",
    "#     df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()*100\n",
    "#     df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)*100\n",
    "#     df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)*100\n",
    "#     df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)*100\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "#     c=[0,0,0,0]\n",
    "    \n",
    "    \n",
    "#     def classify(x):\n",
    "#         percent = 0.06\n",
    "\n",
    "#         if x<-percent:\n",
    "#             c[0]+=1\n",
    "#             return 0\n",
    "#         elif -percent<x<=0:\n",
    "#             c[1]+=1\n",
    "#             return 1\n",
    "#         elif 0<x<percent:\n",
    "#             c[2]+=1\n",
    "#             return 2\n",
    "#         elif percent<x:\n",
    "#             c[3]+=1\n",
    "#             return 3\n",
    "       \n",
    "    \n",
    "#     c=[0,0,0]\n",
    "    \n",
    "    \n",
    "#     def classify(x):\n",
    "#         percent = 0.06\n",
    "\n",
    "#         if x<-percent:\n",
    "#             c[0]+=1\n",
    "#             return 0\n",
    "#         elif -percent<x<=percent:\n",
    "#             c[1]+=1\n",
    "#             return 1\n",
    "#         elif percent<x:\n",
    "#             c[2]+=1\n",
    "#             return 2\n",
    "       \n",
    "    \n",
    "    \n",
    "    c=[0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        #percent = 0.06\n",
    "\n",
    "        if x<0:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif 0<=x:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "\n",
    "        \n",
    "        \n",
    "    legs = 60\n",
    "    \n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = (df[\"Close\"].pct_change(int(legs/10))*100).apply(lambda x: classify(x))\n",
    "\n",
    "    #classified = classified.shift(-1)\n",
    "    classified = classified.shift(-int(legs/10))\n",
    "    \n",
    "    \n",
    "    #df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    #classified=classified[30:len(classified)-2]\n",
    "    df_adjusted=df_feature[30:len(df_feature)-int(legs/10)]\n",
    "    classified=classified[30:len(classified)-int(legs/10)]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    print(classified.isnull().sum())\n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.8)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "#print(y_train.isnull().any())\n",
    "#print(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "#X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "#print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "import pickle\n",
    "filename = 'RF_01.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182900, 193365]\n",
      "class    0\n",
      "dtype: int64\n",
      "0.6437067258495355\n",
      "[[22585 13925]\n",
      " [12885 25852]]\n",
      "[0.63673527 0.64992332]\n",
      "[0.14015376 0.06489428 0.08632392 0.11549055 0.11658106 0.12238688\n",
      " 0.11792953 0.11784886 0.11839116]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "import talib as ta\n",
    "\n",
    "\n",
    "def adjust_data(dataname):\n",
    "    df = pd.read_csv(dataname, index_col='Datetime')\n",
    "\n",
    "    \n",
    "    #以降全ての計算でレート終値を使う\n",
    "    close = np.array(df[\"Close\"])\n",
    "    \n",
    "    \n",
    "    #特徴量を入れるための空のdataframeを作成\n",
    "    df_feature = pd.DataFrame(columns=[ \n",
    "        \"Close\",\n",
    "        \"hour_range\",\n",
    "#         \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "#         \"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\n",
    "        \"day\",\n",
    "#         \"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\n",
    "        \"SMA_hour/current\",\n",
    "        \"SMA_2hour/current\",\n",
    "        \"RSI\",\n",
    "        \"MACD\",\n",
    "        \"BBANDS+2σ\",\n",
    "        \"BBANDS-2σ\"#,\n",
    "#         \"10m_rate\",\n",
    "#         \"20m_rate\",\n",
    "#         \"30m_rate\",\n",
    "#         \"40m_rate\",\n",
    "#         \"50m_rate\",\n",
    "#         \"60m_rate\",\n",
    "#         \"70m_rate\",\n",
    "#         \"80m_rate\",\n",
    "#         \"90m_rate\",\n",
    "#         \"100m_rate\",\n",
    "#         \"110m_rate\",\n",
    "#         \"120m_rate\",\n",
    "#         \"130m_rate\",\n",
    "#         \"140m_rate\",\n",
    "#         \"150m_rate\",\n",
    "#         \"160m_rate\",\n",
    "#         \"170m_rate\",\n",
    "#         \"180m_rate\",\n",
    "#         \"190m_rate\",\n",
    "#         \"200m_rate\",\n",
    "#         \"210m_rate\",\n",
    "#         \"220m_rate\",\n",
    "#         \"230m_rate\",\n",
    "#         \"240m_rate\",\n",
    "#         \"250m_rate\",\n",
    "#         \"260m_rate\",\n",
    "#         \"270m_rate\",\n",
    "#         \"280m_rate\",\n",
    "#         \"290m_rate\",\n",
    "#         \"300m_rate\"\n",
    "        ])\n",
    "\n",
    "\n",
    "    df_feature[\"Close\"] = df[\"Close\"]\n",
    "    df_feature[\"hour_range\"] = df[\"hour_range\"]\n",
    "    \n",
    "#     df_feature[\"January\"] = df[\"January\"]\n",
    "#     df_feature[\"February\"] = df[\"February\"]\n",
    "#     df_feature[\"March\"] = df[\"March\"]\n",
    "#     df_feature[\"April\"] = df[\"April\"]\n",
    "#     df_feature[\"May\"] = df[\"May\"]\n",
    "#     df_feature[\"June\"] = df[\"June\"]\n",
    "#     df_feature[\"July\"] = df[\"July\"]\n",
    "#     df_feature[\"August\"] = df[\"August\"]\n",
    "#     df_feature[\"September\"] = df[\"September\"]\n",
    "#     df_feature[\"October\"] = df[\"October\"]\n",
    "#     df_feature[\"November\"] = df[\"November\"]\n",
    "#     df_feature[\"December\"] = df[\"December\"]\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "    #print(df_feature.isnull().any())\n",
    "    \n",
    "#     days=[\"Sunday\",\"Monday\",\"Thuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "#     for i in days:\n",
    "#         df_feature[i] = df[i]\n",
    "\n",
    "#     df_feature[\"Sunday\"] = df[\"Sunday\"]\n",
    "#     df_feature[\"Monday\"] = df[\"Monday\"]\n",
    "#     df_feature[\"Thuesday\"] = df[\"Thuesday\"]\n",
    "#     df_feature[\"Wednesday\"] = df[\"Wednesday\"]\n",
    "#     df_feature[\"Thursday\"] = df[\"Thursday\"]\n",
    "#     df_feature[\"Friday\"] = df[\"Friday\"]\n",
    "#     df_feature[\"Saturday\"] = df[\"Saturday\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature[\"day\"] = df[\"day\"]\n",
    "    \n",
    "#     i = 10\n",
    "#     while i != 300 + 10:\n",
    "#         df_feature[str(i)+\"m_rate\"] = df[\"Close\"].pct_change(int(i/10))*100\n",
    "#         i += 10\n",
    "#     df_feature[\"10m_rate\"] = df[\"Close\"].pct_change()*100\n",
    "#     df_feature[\"30m_rate\"] = df[\"Close\"].pct_change(3)*100\n",
    "#     df_feature[\"60m_rate\"] = df[\"Close\"].pct_change(6)*100\n",
    "#     df_feature[\"120m_rate\"] = df[\"Close\"].pct_change(12)*100\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #以下、talibを用いてテクニカル指標（今回の学習で用いる特徴量）を算出しdf_feature入れる\n",
    "\n",
    "    #単純移動平均は、単純移動平均値とその日の終値の比を特徴量として用いる\n",
    "    df_feature[\"SMA_hour/current\"]= ta.SMA(close, timeperiod=6) / close\n",
    "    df_feature[\"SMA_2hour/current\"]= ta.SMA(close, timeperiod=12) / close\n",
    "\n",
    "    #RSI\n",
    "    df_feature[\"RSI\"] = ta.RSI(close, timeperiod=12)\n",
    "\n",
    "    #MACD\n",
    "    df_feature[\"MACD\"], _ , _= ta.MACD(close, fastperiod=3, slowperiod=18, signalperiod=9)\n",
    "\n",
    "    #ボリンジャーバンド \n",
    "    upper, middle, lower = ta.BBANDS(close, timeperiod=20, nbdevup=3, nbdevdn=3)\n",
    "    df_feature[\"BBANDS+2σ\"] = upper / close\n",
    "    df_feature[\"BBANDS-2σ\"] = lower / close\n",
    "\n",
    "    \n",
    "    \n",
    "#     c=[0,0,0,0]\n",
    "    \n",
    "    \n",
    "#     def classify(x):\n",
    "#         percent = 0.06\n",
    "\n",
    "#         if x<-percent:\n",
    "#             c[0]+=1\n",
    "#             return 0\n",
    "#         elif -percent<x<=0:\n",
    "#             c[1]+=1\n",
    "#             return 1\n",
    "#         elif 0<x<percent:\n",
    "#             c[2]+=1\n",
    "#             return 2\n",
    "#         elif percent<x:\n",
    "#             c[3]+=1\n",
    "#             return 3\n",
    "       \n",
    "    \n",
    "#     c=[0,0,0]\n",
    "    \n",
    "    \n",
    "#     def classify(x):\n",
    "#         percent = 0.06\n",
    "\n",
    "#         if x<-percent:\n",
    "#             c[0]+=1\n",
    "#             return 0\n",
    "#         elif -percent<x<=percent:\n",
    "#             c[1]+=1\n",
    "#             return 1\n",
    "#         elif percent<x:\n",
    "#             c[2]+=1\n",
    "#             return 2\n",
    "       \n",
    "    \n",
    "    \n",
    "    c=[0,0]\n",
    "    \n",
    "    \n",
    "    def classify(x):\n",
    "        #percent = 0.06\n",
    "\n",
    "        if x<0:\n",
    "            c[0]+=1\n",
    "            return 0\n",
    "        elif 0<=x:\n",
    "            c[1]+=1\n",
    "            return 1\n",
    "\n",
    "        \n",
    "        \n",
    "    legs = 60\n",
    "    \n",
    "    classified = pd.DataFrame(columns=[\"class\"])\n",
    "    classified[\"class\"] = (df[\"Close\"].pct_change(int(legs/10))*100).apply(lambda x: classify(x))\n",
    "\n",
    "    #classified = classified.shift(-1)\n",
    "    classified = classified.shift(-int(legs/10))\n",
    "    \n",
    "    \n",
    "    #df_adjusted=df_feature[30:len(df_feature)-2]\n",
    "    #classified=classified[30:len(classified)-2]\n",
    "    df_adjusted=df_feature[30:len(df_feature)-int(legs/10)]\n",
    "    classified=classified[30:len(classified)-int(legs/10)]\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    print(classified.isnull().sum())\n",
    "    \n",
    "    \n",
    "    return df_adjusted, classified\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "\n",
    "df_adjusted, classified = adjust_data(\"./2010_1to2019_12.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adjusted, classified[\"class\"], train_size=0.8)\n",
    "\n",
    "\n",
    "#y_ravel=np.ravel(y_train)\n",
    "#print(X_train.isnull().any())\n",
    "#print(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "#X_real, y_real = adjust_data(\"./USD_JPY_202009_M10.csv\")\n",
    "#print(accuracy_score(y_real, clf.predict(X_real)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, clf.predict(X_test), average=None))\n",
    "\n",
    "import pickle\n",
    "filename = 'RF_01.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# print(X_train)\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
